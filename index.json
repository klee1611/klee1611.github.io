[{"categories":["Concurrency Programming"],"content":"After Python 3.4, Asyncio emerged, which can improve performance in specific scenarios. Combined with the pre-existing Multiprocessing and Multithreading, I have compiled a few records on the principles, differences, and use cases for these three technologies. This first post will briefly introduce the basic concepts and suitable scenarios for each of the three. ","date":"2025-10-25","objectID":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/:0:0","tags":["Python","Concurrency","Parallelism","Multiprocessing","Multithreading","Asyncio","Coroutine","GIL","Performance"],"title":"Multiprocessing, Multithreading and Asyncio in Python Part 1 - Basic Concept","uri":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Multiprocessing A program can execute multiple independent processes simultaneously. Each process has its own independent memory space, and therefore can completely bypass the limitations of Python’s GIL (Global Interpreter Lock). This means they can truly execute in parallel on multi-core CPUs, independently and without interference. Use Case: CPU-bound tasks, such as extensive mathematical calculations, data processing, image recognition, etc. It can effectively utilize the computing power of multi-core CPUs. import multiprocessing import time def cpu_bound_task(n): count = 0 for i in range(n): count += i print(f\"Finished task with {n}\") if __name__ == '__main__': start_time = time.time() processes = [] for i in range(4): p = multiprocessing.Process(target=cpu_bound_task, args=(10**7,)) processes.append(p) p.start() for p in processes: p.join() end_time = time.time() print(f\"Multiprocessing took {end_time - start_time:.2f} seconds.\") Pros: Can achieve true parallelism using multi-core CPUs. Not limited by the GIL. Independent memory space between processes leads to high stability and less likelihood of Race Conditions. Cons: Creating independent processes requires more resources. Inter-process communication (IPC) is more complex, requiring mechanisms like Queue, Pipe, etc. ","date":"2025-10-25","objectID":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/:1:0","tags":["Python","Concurrency","Parallelism","Multiprocessing","Multithreading","Asyncio","Coroutine","GIL","Performance"],"title":"Multiprocessing, Multithreading and Asyncio in Python Part 1 - Basic Concept","uri":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Multithreading Multiple threads are created within a single process. These threads share the same memory space used by the process, allowing for easy data sharing and exchange. It is worth noting that, before a certain version, unlike other programming languages such as C/C++, Python’s multithreading was limited by the Python GIL (Global Interpreter Lock). Even when running on a multi-core CPU, Python’s multithreading could not actually achieve parallel computing. As the number of Python users grew, demands like PEP-703 emerged. Starting from Python 3.13, an experimental feature to optionally disable the GIL (free-threading mode) was included. Python 3.14 introduced a GIL-free, Free-threaded version. GIL (Global Interpreter Lock): The GIL is a mechanism in CPython (the official Python implementation) designed to protect Python objects from being corrupted. This mechanism ensures that only one thread can execute Python bytecode at any given time. This means that for CPU-bound tasks, even on a multi-core CPU, Python’s multithreading cannot achieve true parallel computing. However, when a thread encounters an I/O operation (like reading/writing a file or a network request), it releases the GIL, allowing other threads a chance to run. Therefore, Multithreading is more suitable for handling I/O-bound tasks. Use Case: I/O-bound tasks, such as web scraping, file downloads, API requests, etc. import threading import requests import time def io_bound_task(url): try: response = requests.get(url) print(f\"Downloaded {url} with status {response.status_code}\") except Exception as e: print(f\"Error downloading {url}: {e}\") if __name__ == '__main__': urls = [\"https://www.google.com\"] * 5 start_time = time.time() threads = [] for url in urls: t = threading.Thread(target=io_bound_task, args=(url,)) threads.append(t) t.start() for t in threads: t.join() end_time = time.time() print(f\"Multithreading took {end_time - start_time:.2f} seconds.\") Pros: The overhead of creating a thread is smaller than that of a process. Shared memory makes data exchange convenient. Cons: Limited by the GIL, cannot utilize multi-core CPUs for CPU-bound tasks. Requires handling thread synchronization issues, such as using Lock to avoid Race conditions. ","date":"2025-10-25","objectID":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/:2:0","tags":["Python","Concurrency","Parallelism","Multiprocessing","Multithreading","Asyncio","Coroutine","GIL","Performance"],"title":"Multiprocessing, Multithreading and Asyncio in Python Part 1 - Basic Concept","uri":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Asyncio I/O and Coroutines Asyncio is a standard library introduced after Python 3.4. Conceptually, it uses an Event Loop and Coroutines to achieve concurrency on a single thread. A Coroutine can be seen as a lightweight thread. It can be controlled to pause at a certain point to wait for a task to complete, while simultaneously returning control to the event loop to execute other coroutines. When the condition for the pause is met (e.g., the awaited I/O operation is complete), the event loop will return to continue executing that coroutine. Besides being able to execute other coroutines during the pause, it also saves the overhead of OS-level thread switching, which can significantly improve performance. Use Case: Highly concurrent I/O-bound tasks, especially scenarios that require handling a large number of network connections simultaneously (like Web servers, chat applications). import asyncio import aiohttp import time async def async_io_bound_task(session, url): try: async with session.get(url) as response: print(f\"Downloaded {url} with status {response.status}\") except Exception as e: print(f\"Error downloading {url}: {e}\") async def main(): urls = [\"https://www.google.com\"] * 5 start_time = time.time() async with aiohttp.ClientSession() as session: tasks = [async_io_bound_task(session, url) for url in urls] await asyncio.gather(*tasks) end_time = time.time() print(f\"Asyncio took {end_time - start_time:.2f} seconds.\") if __name__ == '__main__': asyncio.run(main()) Pros: Extremely low thread switching overhead, capable of handling a large number of I/O operations with high efficiency. Operates on a single thread, so there are no race condition issues. Cons: Not suitable for CPU-bound tasks. Requires using async/await syntax and corresponding asynchronous library support (like aiohttp). ","date":"2025-10-25","objectID":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/:3:0","tags":["Python","Concurrency","Parallelism","Multiprocessing","Multithreading","Asyncio","Coroutine","GIL","Performance"],"title":"Multiprocessing, Multithreading and Asyncio in Python Part 1 - Basic Concept","uri":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Comparison Summary Feature Multiprocessing Multithreading Asyncio Basic Unit Process Thread Coroutine Memory Space Independent Shared Shared (Single-threaded) GIL Impact None, can be bypassed Restricted None (Single-threaded) Parallelism/Concurrency Parallelism Concurrency Concurrency Use Case CPU-bound I/O-bound Highly concurrent I/O-bound Pros Can utilize multi-core Shared memory Extremely high I/O efficiency, low overhead Cons High resource overhead, complex IPC GIL limitation, has race conditions Not for CPU-bound tasks If your task is CPU-bound, requiring a lot of CPU computation, then multiprocessing is the most suitable choice because it allows you to fully utilize the power of multi-core CPUs. If your task is IO-bound and the logic is relatively simple, multithreading is a good choice because it is more lightweight than multiprocessing. If your task is IO-bound and requires handling a large number of concurrent connections (e.g., developing a Web server or API), then asyncio is the most efficient. ","date":"2025-10-25","objectID":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/:4:0","tags":["Python","Concurrency","Parallelism","Multiprocessing","Multithreading","Asyncio","Coroutine","GIL","Performance"],"title":"Multiprocessing, Multithreading and Asyncio in Python Part 1 - Basic Concept","uri":"/posts/python-concurrency-parallelism-multiprocessing-multithreading-asyncio.html/"},{"categories":["Tools"],"content":"I originally used Notion as my note-taking software. It’s feature-rich and has a beautiful interface. However, a few years ago, a privacy controversy arose around Notion, accusing them of looking at a company’s content stored in Notion, and even proposing a partnership based on that information. So, I switched to Joplin for a while, but eventually moved to Obsidian, which has a large number of plugins, strong community support, and is highly customizable. Joplin natively supports WebDAV synchronization. After switching to Obsidian, I found the “Remotely Save” plugin, which also supports WebDAV. This allows me to store my notes on my own NAS and keep them synchronized across different devices. ","date":"2024-12-25","objectID":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/:0:0","tags":["WebDAV","Synology","Obsidian","Joplin"],"title":"Sync Obsidian / Joplin Data Across Multiple Devices with Synology WebDAV","uri":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/"},{"categories":["Tools"],"content":"WebDAV and HTTPS Certificate Setup - Connecting to WebDAV Server on an Internal NAS Through a Router ","date":"2024-12-25","objectID":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/:1:0","tags":["WebDAV","Synology","Obsidian","Joplin"],"title":"Sync Obsidian / Joplin Data Across Multiple Devices with Synology WebDAV","uri":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/"},{"categories":["Tools"],"content":"Equipment Router: Synology RT2600AC NAS: Synology 918+ ","date":"2024-12-25","objectID":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/:1:1","tags":["WebDAV","Synology","Obsidian","Joplin"],"title":"Sync Obsidian / Joplin Data Across Multiple Devices with Synology WebDAV","uri":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/"},{"categories":["Tools"],"content":"Environment Setup After installing the WebDAV Server package on the Synology NAS, for security reasons, I disabled HTTP connections in the WebDAV settings and only enabled HTTPS access. (Although I’m not entirely sure, the mobile version of Joplin seems to be restricted to HTTPS for security reasons as well). Then, I configured port forwarding on my router as shown in the image below. Since I had already set up DDNS for the router, I could already connect to the router via a URL. So I went ahead and tested the WebDAV connection. In Joplin’s connection settings, I selected WebDAV, and entered the URL, port number, and the folder to store the notes in the WebDAV URL field (e.g., https://.....:5xxx/homes/xxx/xxx/xxx). After clicking the test connection button, I found a certificate issue. After some googling, I discovered that this was because the domain name in the connection URL corresponded to the router, which then forwarded the connection to the NAS. However, the NAS did not have a certificate for that domain name, causing a mismatch between the certificate and the URL when Joplin tried to connect. (You can use https://www.geocerts.com/ssl-checker to check this). So, I exported the certificate from the router and imported it to the NAS. Then, in the NAS settings, I found the certificate configuration and changed the WebDAV certificate to the one imported from the router, as shown in the image below. Then I went to https://www.geocerts.com/ssl-checker and ran another test. This time, I confirmed that the certificate for the port connected via HTTPS was correct. I went back to Joplin’s connection settings and tested the connection again to confirm it was successful, and synchronization worked smoothly. ","date":"2024-12-25","objectID":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/:1:2","tags":["WebDAV","Synology","Obsidian","Joplin"],"title":"Sync Obsidian / Joplin Data Across Multiple Devices with Synology WebDAV","uri":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/"},{"categories":["Tools"],"content":"Supplementary Information - Notion Privacy Controversy In a 2020 post in the Notion.Taiwan Official Community, someone mentioned that Notion would look at user data and even propose partnerships: In the end, Notion’s official statement removed the controversial “Business Development and Strategic Partnerships” clause. ","date":"2024-12-25","objectID":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/:2:0","tags":["WebDAV","Synology","Obsidian","Joplin"],"title":"Sync Obsidian / Joplin Data Across Multiple Devices with Synology WebDAV","uri":"/posts/sync-obsidian-joplin-data-across-multiple-device-synology-webdav.html/"},{"categories":["Develop environment"],"content":"Today I encountered a problem: After installing nvm, the path for installing global packages changed, making it impossible to directly remove previously installed global packages using npm uninstall -g. How did I discover this? A long time ago, I installed a global package that could be executed directly from the terminal using a command. But because it was so long ago, when I tried to upgrade that package, I found it wasn’t listed in npm list -g. So, I first used which to find its location, then discovered it was a symbolic link and used ls -al to see where that link pointed. I found it was under /usr/lib/node_modules, which clearly indicated it was installed with npm -g. Then I carefully re-examined the output of npm list -g, and found that other global packages were listed under /Users/\u003cUSER_NAME\u003e/.nvm/versions/node/v16.5.0/lib. After some Googling, I found a way to list the global packages installed before nvm using nvm use system \u0026\u0026 npm ls -g --depth=0. Tragically, it showed: System version of node not found. It seems I had already removed node from the system… So, I found another command, nvm deactivate, to temporarily disable nvm. Then, I reinstalled node using brew. After that, I ran npm list -g again, and finally saw the package that was installed before nvm!!! Hooray!! I could finally successfully remove/upgrade the previously installed global package. After resolving the issue, to bring nvm back, simply restart the shell with source ~/.zshrc or similar. ","date":"2021-11-06","objectID":"/posts/managing-pre-exist-global-npm-packages-after-installing-nvm.html/:0:0","tags":["NVM","NPM","Node.js","Global Packages","Troubleshooting"],"title":"Managing Pre-existing Global NPM Packages After Installing NVM","uri":"/posts/managing-pre-exist-global-npm-packages-after-installing-nvm.html/"},{"categories":["Develop environment"],"content":"Functions and Reasons for Using pyenv pyenv is a tool used to install various versions of Python on a system, and to conveniently switch between Python versions. When you need to develop or maintain projects that require different Python versions simultaneously, you will need to use pyenv to help switch Python versions. New Python versions usually include syntax updates or new features. For example, Python’s async / await feature appeared only in Python 3.5 and later. Projects developed with Python versions below 3.5 cannot use it. Another example is having projects that use both Python 2 and Python 3. Since Python 2 and Python 3 are syntactically incompatible, it is necessary to install both Python 2 and Python 3 on the system. In such cases, pyenv can be used to conveniently switch Python versions. ","date":"2021-11-01","objectID":"/posts/pyenv-notes.html/:0:1","tags":["Programming","Python","Pyenv"],"title":"Pyenv Notes","uri":"/posts/pyenv-notes.html/"},{"categories":["Develop environment"],"content":"Installation and Initialization Installation brew install pyenv After installation, run initialization pyenv init Then, follow the instructions to paste the displayed code into ~/.zshrc or ~/.bash_profile ","date":"2021-11-01","objectID":"/posts/pyenv-notes.html/:0:2","tags":["Programming","Python","Pyenv"],"title":"Pyenv Notes","uri":"/posts/pyenv-notes.html/"},{"categories":["Develop environment"],"content":"Common Commands List available Python versions for installation pyenv install --list This will show: Available versions: 2.1.3 2.2.3 2.3.7 2.4.0 ... 3.9.6 3.9.7 3.10.0 3.10-dev 3.11.0a1 ... Install a specific Python version pyenv install 3.10.0 Observe which Python versions have been installed pyenv versions This will show: * system (set by ......./.pyenv/version) 3.10.0 This indicates that the system’s default version and the recently installed 3.10.0 are available, but the currently used Python version is the system’s default. Switch the system-wide Python version pyenv global 3.10.0 Running pyenv versions anywhere in the system will show that the currently used Python version is 3.10.0. Switch the Python version only for the current directory pyenv local 3.7.12 In the current directory, pyenv versions will show that version 3.7.12 is being used. However, in other directories, if a version was previously set using pyenv global (e.g., 3.10.0), then pyenv versions will show the version set by pyenv global (3.10.0). If pyenv global was not run to set a version, pyenv versions will show the system’s default version. Uninstall a specific Python version pyenv uninstall 3.7.12 ","date":"2021-11-01","objectID":"/posts/pyenv-notes.html/:0:3","tags":["Programming","Python","Pyenv"],"title":"Pyenv Notes","uri":"/posts/pyenv-notes.html/"},{"categories":["Develop environment"],"content":"Reference pyenv Github ","date":"2021-11-01","objectID":"/posts/pyenv-notes.html/:1:0","tags":["Programming","Python","Pyenv"],"title":"Pyenv Notes","uri":"/posts/pyenv-notes.html/"},{"categories":["Concurrency Programming"],"content":"Before the advent of asyncio, when a Python program had many tasks that needed to be executed concurrently, and wanted to improve program performance, the only options were multiprocessing or threading. After Python 3.4, asyncio became another option. asyncio can be used to write coroutines, and execute coroutines concurrently using an event loop, reducing unnecessary waiting time in the program to improve performance. ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:0:0","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Coroutines ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:1:0","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Coroutine Definition In the Python official documentation, Python coroutines are defined as: Coroutines are a more generalized form of subroutines. Subroutines are entered at one point and exited at another point. Coroutines can be entered, exited, and resumed at many different points. They can be implemented with the async def statement. See also PEP 492. This means that Python coroutines are quite similar to subroutines. The difference is that a subroutine executes from start to finish in one go, and then terminates. A coroutine, on the other hand, can execute up to a certain point, pause, and then resume execution later. ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:1:1","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Defining and Executing Coroutines with async, await, and asyncio.run async can be used to define a coroutine. By adding async before the def keyword when defining a function, you can define a coroutine using async def. await is used to define a suspension point in a coroutine. When await is encountered, the coroutine can pause, and then resume execution later. await can only be followed by an awaitable object. Awaitable objects include coroutines or event loop tasks, etc. import asyncio async def ten_sec_sleep(): await asyncio.sleep(10) print('10 sec sleep finish') if __name__ == '__main__': asyncio.run(ten_sec_sleep()) ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:1:2","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Event Loop ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:2:0","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"What is an event loop In the Python official documentation, the event loop is introduced as: The event loop is the core of every asyncio application. Event loops run asynchronous tasks and callbacks, perform network IO operations, and run subprocesses. Simply put, it is used to run asynchronously executing tasks. An event loop executes only one task at a time. When running coroutines using an event loop, when a task reaches a programmer-defined suspension point, the event loop pauses and schedules that task, then switches to execute other work (which could be other tasks or callbacks, etc.). This makes the combination of event loop and coroutine particularly suitable for handling I/O-bound tasks; by defining the I/O operations of a coroutine as suspension points, and using an event loop to run these coroutines, the time spent waiting for I/O can be used to perform other work. ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:2:1","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Executing Coroutines with an Event Loop Executing a single coroutine: import asyncio async def ten_sec_sleep(count): await asyncio.sleep(10) print(f'10 sec sleep finish, count: {count}') if __name__ == '__main__': loop = asyncio.get_event_loop() task = loop.create_task(ten_sec_sleep(0)) loop.run_until_complete(task) The execution time using the time command is 10.09 seconds. 10 sec sleep finish, count: 0 10.09 real 0.06 user 0.01 sys Executing multiple coroutines concurrently: import asyncio async def ten_sec_sleep(count): await asyncio.sleep(10) print(f'10 sec sleep finish, count: {count}') if __name__ == '__main__': loop = asyncio.get_event_loop() tasks = [] for i in range(10): tasks.append(loop.create_task(ten_sec_sleep(i))) loop.run_until_complete(asyncio.wait(tasks)) Whenever sleep(10) is executed, the event loop can switch to another coroutine to execute. The concurrent execution time for coroutines using the time command is 10.09 seconds. 10 sec sleep finish, count: 0 10 sec sleep finish, count: 1 10 sec sleep finish, count: 2 10 sec sleep finish, count: 3 10 sec sleep finish, count: 4 10 sec sleep finish, count: 5 10 sec sleep finish, count: 6 10 sec sleep finish, count: 7 10 sec sleep finish, count: 8 10 sec sleep finish, count: 9 10.09 real 0.07 user 0.01 sys ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:2:2","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Concurrency Programming"],"content":"Performance Measurement A program that continuously sends 10 requests to Google, without using coroutines: import requests def issue_req(count): resp = requests.get('http://www.google.com.tw') print(f'count: {count}, resp status: {resp.status_code}') if __name__ == '__main__': for i in range(10): issue_req(i) The time required using the time command is 0.83 seconds. count: 0, resp status: 200 count: 1, resp status: 200 count: 2, resp status: 200 count: 3, resp status: 200 count: 4, resp status: 200 count: 5, resp status: 200 count: 6, resp status: 200 count: 7, resp status: 200 count: 8, resp status: 200 count: 9, resp status: 200 0.83 real 0.16 user 0.05 sys Using coroutines to send requests concurrently: import requests import asyncio async def issue_req(count): loop = asyncio.get_event_loop() resp = await loop.run_in_executor( None, requests.get, 'http://www.google.com.tw' ) print(f'count: {count}, resp status: {resp.status_code}') if __name__ == '__main__': loop = asyncio.get_event_loop() tasks = [] for i in range(10): tasks.append(loop.create_task(issue_req(i))) loop.run_until_complete(asyncio.wait(tasks)) The time required using the time command is 0.31 seconds. count: 0, resp status: 200 count: 2, resp status: 200 count: 3, resp status: 200 count: 7, resp status: 200 count: 5, resp status: 200 count: 1, resp status: 200 count: 9, resp status: 200 count: 6, resp status: 200 count: 8, resp status: 200 count: 4, resp status: 200 0.31 real 0.18 user 0.05 sys Looking at these 10 requests, from 0.83 seconds to 0.31 seconds, the performance improved by (0.83 - 0.31)/0.83 * 100% = 62.65%, which is quite significant. For programs with many I/O-bound tasks, using coroutines is a good choice. ","date":"2021-10-27","objectID":"/posts/python-coroutine-asyncio.html/:3:0","tags":["Programming","Python","Concurrent Processing"],"title":"Python Coroutine Asyncio","uri":"/posts/python-coroutine-asyncio.html/"},{"categories":["Develop environment"],"content":"Why Pipenv When maintaining many Python projects, different projects might use different versions of the same Python libraries. Not using a virtual environment and installing all Python modules directly on your machine will lead to version conflicts. In the past, the mechanism of virtualenv + requirements.txt allowed different projects to use different versions of the same package, and also enabled new developers or production environments to quickly install the packages required by the project. However, updating packages was quite troublesome, requiring manual re-exporting of a new requirements.txt. Furthermore, when a project had different environment requirements (e.g., development environment and production environment), two sets of package configurations, requirements-prod.txt and requirements-dev.txt, had to be maintained. Without pyenv, it was also impossible to switch between different Python versions. Later, the officially recommended pipenv from Python solved these problems, making it convenient to achieve the following with just commands: Create independent Python versions and package virtual environments. Install and record package versions in automatically generated Pipfile and Pipfile.lock, while checking package security through package hash values. Record package usage environments (separating development and production environments). Read .env files to set environment variables for the virtual environment. Automatically switch Python versions in the system (or Python versions installed with pyenv). ","date":"2021-09-26","objectID":"/posts/pipenv-notes.html/:1:0","tags":["Programming","Python","Pipenv"],"title":"Pipenv Notes","uri":"/posts/pipenv-notes.html/"},{"categories":["Develop environment"],"content":"Operating Pipenv ","date":"2021-09-26","objectID":"/posts/pipenv-notes.html/:2:0","tags":["Programming","Python","Pipenv"],"title":"Pipenv Notes","uri":"/posts/pipenv-notes.html/"},{"categories":["Develop environment"],"content":"Install Pipenv pip3 install pipenv ","date":"2021-09-26","objectID":"/posts/pipenv-notes.html/:2:1","tags":["Programming","Python","Pipenv"],"title":"Pipenv Notes","uri":"/posts/pipenv-notes.html/"},{"categories":["Develop environment"],"content":"Pipenv Commands Create an independent virtual environment for a specific Python version: Navigate to the project directory. pipenv --python 3.8 Note that the specified Python version must be available on the system; otherwise, you need to install it using pyenv. Install packages pipenv install flask Install development packages Packages with --dev will be placed under [dev-packages] in the automatically generated Pipfile. pipenv install pytest --dev Uninstall packages pipenv uninstall flask Execute scripts in the created virtual environment pipenv run python server.py Other commands like pytest can also be executed. pipenv run pytest Enter the virtual environment pipenv shell To exit the virtual environment, simply type exit. Create a virtual environment using existing Pipfile and Pipfile.lock pipenv install To install development environment packages as well: pipenv install --dev Create Pipfile and Pipfile.lock from requirements.txt pipenv install Output requirements.txt This is generally not necessary, but in some special cases (e.g., requirements for specific platforms), it can still be done. pipenv lock --requirements \u003e requirements.txt Upgrade packages in the virtual environment pipenv update Delete the current virtual environment pipenv --rm ","date":"2021-09-26","objectID":"/posts/pipenv-notes.html/:2:2","tags":["Programming","Python","Pipenv"],"title":"Pipenv Notes","uri":"/posts/pipenv-notes.html/"},{"categories":["Develop environment"],"content":"Upgrade Pipenv pip3 install --upgrade pipenv","date":"2021-09-26","objectID":"/posts/pipenv-notes.html/:2:3","tags":["Programming","Python","Pipenv"],"title":"Pipenv Notes","uri":"/posts/pipenv-notes.html/"},{"categories":["Web"],"content":"Stateless HTTP HTTP is a stateless protocol, meaning that each request/response is independent, and is unrelated to previous or subsequent requests/responses. The same request will always receive the same response, and will not differ based on the content of previous requests/responses. This allows the server to save a large amount of database and server storage space because it doesn’t need to store user information. It also speeds up response times and saves considerable network bandwidth because the client doesn’t always have to connect to the same socket. However, when a website needs to perform continuous actions (e.g., requiring user authentication), some mechanisms are needed to assist. At this point, most websites utilize sessions or cookies. ","date":"2021-06-28","objectID":"/posts/stateless-http-stateful-session-and-cookies.html/:1:0","tags":["Web","Session","Cookie"],"title":"Stateless HTTP, Stateful Session and Cookies","uri":"/posts/stateless-http-stateful-session-and-cookies.html/"},{"categories":["Web"],"content":"Session A session is a stateful period of time. HTTP requests/responses are stateless, but if state information is carried through stateless requests/responses, the client and server can create stateful operations using the state information carried in the requests/responses. For example, if a certain action requires the user to be logged in and have selected option A before it can be performed, it is desirable to have a stateful period (session) that represents the state “user is logged in and has selected option A.” There are many ways to achieve this state. For instance, during this period, requests can carry an encrypted user ID and option A to inform the server that the user is logged in, their identity, and the selected option. There are many ways to implement a session, the most common being cookies. However, cookies are just one method; it doesn’t mean that sessions can only be implemented through cookies. Sessions can also be created through other means, such as using query strings to record previous interactions. ","date":"2021-06-28","objectID":"/posts/stateless-http-stateful-session-and-cookies.html/:2:0","tags":["Web","Session","Cookie"],"title":"Stateless HTTP, Stateful Session and Cookies","uri":"/posts/stateless-http-stateful-session-and-cookies.html/"},{"categories":["Web"],"content":"Cookie A cookie is a mechanism for implementing sessions. The server can use the Set-Cookie header to instruct the browser to set a cookie and specify its content. Subsequently, when the browser sends a request to the same domain and path, it will include the cookie with the request. This way, when certain states need to be remembered, the server only needs to instruct the browser to set the necessary cookie. Then, when a request is sent, the server can understand the current state by examining the cookie’s content. Since the content of a cookie can be modified by the user, for security considerations, there are two common approaches when using cookies (they can also be used together): ","date":"2021-06-28","objectID":"/posts/stateless-http-stateful-session-and-cookies.html/:3:0","tags":["Web","Session","Cookie"],"title":"Stateless HTTP, Stateful Session and Cookies","uri":"/posts/stateless-http-stateful-session-and-cookies.html/"},{"categories":["Web"],"content":"Cookie-based session Encrypt the cookie content. The server decrypts the content after receiving it to understand what the cookie stores. Continuing the example above, the user ID and option A would be encrypted and placed in the cookie. Points to note: Because cookies have a size limit, special attention must be paid to ensure the encrypted cookie size is not too large. The encryption key must be properly secured. ","date":"2021-06-28","objectID":"/posts/stateless-http-stateful-session-and-cookies.html/:3:1","tags":["Web","Session","Cookie"],"title":"Stateless HTTP, Stateful Session and Cookies","uri":"/posts/stateless-http-stateful-session-and-cookies.html/"},{"categories":["Web"],"content":"Session ID Use an ID (Session Identifier, Session ID) to record user identity, while all other data (Session Data) is stored on the server. Continuing the example above, the user’s selected option A would be stored on the server, and the user’s ID would be placed in the cookie. Points to note: The Session ID must be designed to be difficult to guess; if it is guessed, the user’s identity will be stolen. If the website is not secure enough, and the Session ID is stolen by another malicious website or hacker on a certain page, the user’s identity will be stolen. ","date":"2021-06-28","objectID":"/posts/stateless-http-stateful-session-and-cookies.html/:3:2","tags":["Web","Session","Cookie"],"title":"Stateless HTTP, Stateful Session and Cookies","uri":"/posts/stateless-http-stateful-session-and-cookies.html/"},{"categories":["Develop environment"],"content":" Bringing the terminal settings from Linux and Mac to Windows for easier operation. ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:0:0","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"Windows Terminal Features With Windows Terminal, you can: Enable multiple tabs (quickly switch between multiple Linux CLIs, Windows CLIs, PowerShell, etc.) Customize key bindings (shortcuts for opening/closing tabs, copy/paste, etc.) Use search functionality Customize themes These features offer much more than native WSL support, and allow for a setup similar to my Linux or Mac development environments, which is why I decided to use Windows Terminal. ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:1:0","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"Windows Terminal Settings After searching for and installing Windows Terminal from the Microsoft Store, you can start configuring it. ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:2:0","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"Setting WSL as the Default Opening Environment for Windows Terminal In the Windows Terminal’s [V] arrow menu, select “Settings,” which will open a JSON file for modification. From the profiles \u003e list, find the Linux distribution you want to set as default, for example: { \"guid\": \"{xxxxxxxxxxxxxxx}\", \"hidden\": false, \"name\": \"Ubuntu-18.04\", \"commandline\": \"wsl.exe\", \"source\": \"Windows.Terminal.Wsl\" } Copy the GUID string enclosed in curly braces after guid, and replace the ID of the originally default profile with that ID: \"defaultProfile\": \"{yyyyyy}\" (Replace yyyyyy with the GUID of your Linux distribution) ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:2:1","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"Setting the Default Starting Directory for Windows Terminal In the Linux distribution profile within the JSON settings file, append the default directory to open (~ refers to the user’s Linux home directory) to the commandline: \"commandline\": \"wsl.exe ~\", ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:2:2","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"Setting the Windows Terminal Scheme Add this line to the Linux distribution profile in the JSON settings file: \"colorScheme\": \"One Half Dark\", One Half Dark is one of the color schemes provided by Windows. Other schemes can be found in Microsoft Doc: Color schemes in Windows Terminal. ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:2:3","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"Setting the Windows Terminal Font Add this to the Linux distribution profile in the JSON settings file: \"fontFace\": \"xxxxx\", xxxxx is the name of the font. If you need to use Powerline, you can first install Powerline fonts, then fill in the desired font name. ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:2:4","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"Reference Microsoft Doc: Install and set up Windows Terminal Microsoft Doc: Color schemes in Windows Terminal Set Windows Terminal as WSL operating interface ","date":"2021-04-11","objectID":"/posts/wsl-2-on-windows-part-2.html/:3:0","tags":["WSL","Ubuntu","Windows terminal"],"title":"WSL 2 on Windows Part 2 - Terminal Interface Settings","uri":"/posts/wsl-2-on-windows-part-2.html/"},{"categories":["Develop environment"],"content":"I’m used to using Linux or Mac terminals for work. I took some time to set up the WSL environment on my home PC to easily switch work environments. ","date":"2021-04-10","objectID":"/posts/wsl-2-on-windows-part-1.html/:0:0","tags":["WSL","Ubuntu"],"title":"WSL 2 on Windows Part 1 - Installation and Activation","uri":"/posts/wsl-2-on-windows-part-1.html/"},{"categories":["Develop environment"],"content":"Differences between WSL 2 and WSL 1 WSL 2 is based on Hyper-V and runs a full Linux kernel in a virtual machine. WSL 1 is a simulation of Linux functionalities on the Windows system. Therefore, WSL 2 supports more native Linux features and system calls than WSL 1. If you need to use low-level Linux applications, WSL 2 offers better support than WSL 1. Generally, WSL 2 also has better performance for starting processes, except when reading files from the host system. However, because WSL 2 runs a Linux kernel in a VM, its integration with Windows as the host is relatively poorer than WSL 1. Processes within WSL 2 cannot be managed by Windows Task Manager, and there’s an additional layer in the network connection between Windows and WSL 2. Due to WSL 2’s use of Hyper-V, I’ve heard reports of incompatibility issues with VMWare. I don’t use VMWare, so I don’t know if this issue is real, but I haven’t encountered any problems when using Docker. Microsoft Doc lists a detailed comparison of WSL 1 and WSL 2. ","date":"2021-04-10","objectID":"/posts/wsl-2-on-windows-part-1.html/:1:0","tags":["WSL","Ubuntu"],"title":"WSL 2 on Windows Part 1 - Installation and Activation","uri":"/posts/wsl-2-on-windows-part-1.html/"},{"categories":["Develop environment"],"content":"Requirements Windows version must be Windows 10. If your version is lower, please use Windows Update: For X64 systems: Version 1903 or higher, with Build 18362 or higher. For ARM64 systems: Version 2004 or higher, with Build 19041 or higher. The machine must have virtualization features enabled. This can usually be found in the motherboard’s BIOS settings. Look for settings related to CPU configuration, such as Intel Virtualization, and enable it. ","date":"2021-04-10","objectID":"/posts/wsl-2-on-windows-part-1.html/:2:0","tags":["WSL","Ubuntu"],"title":"WSL 2 on Windows Part 1 - Installation and Activation","uri":"/posts/wsl-2-on-windows-part-1.html/"},{"categories":["Develop environment"],"content":"Installing and Activating WSL 2 Open PowerShell with administrator privileges. Enable Windows Subsystem for Linux: Run dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart Enable Virtual Machine Platform optional feature: Run dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Restart your computer. Download and install the WSL 2 Linux kernel update package. Set WSL 2 as the default version: wsl --set-default-version 2 Go to the Microsoft Store, find the Linux distribution you want to install, and install it, setting up your account and password. You can check the WSL version of the installed Linux distribution in PowerShell: wsl -l -v You can also change the WSL version of a Linux distribution: wsl --set-version \u003cdistribution name\u003e \u003cversionNumber\u003e ","date":"2021-04-10","objectID":"/posts/wsl-2-on-windows-part-1.html/:3:0","tags":["WSL","Ubuntu"],"title":"WSL 2 on Windows Part 1 - Installation and Activation","uri":"/posts/wsl-2-on-windows-part-1.html/"},{"categories":["Develop environment"],"content":"Reference Microsoft WSL 2 Installation Guide Microsoft DOC: Compare WSL 1 and WSL 2 ","date":"2021-04-10","objectID":"/posts/wsl-2-on-windows-part-1.html/:4:0","tags":["WSL","Ubuntu"],"title":"WSL 2 on Windows Part 1 - Installation and Activation","uri":"/posts/wsl-2-on-windows-part-1.html/"},{"categories":["Programming"],"content":"Shallow Copy Copies as little as possible. A new structure created by a shallow copy has the same structure as the old one, and they share the memory address of elements. For example, in Java: int[] arr1 = {1, 2, 3}; int[] arr2 = arr1; arr2 is a shallow copy of arr1. If one of the structures modifies an element, the other will also be affected. ","date":"2020-01-21","objectID":"/posts/deep-copy-shallow-copy.html/:0:1","tags":["Programming"],"title":"Deep Copy and Shallow Copy","uri":"/posts/deep-copy-shallow-copy.html/"},{"categories":["Programming"],"content":"Deep Copy Copies everything. A new structure created by a deep copy not only has the same structure as the old one, but also copies all elements of the old structure to the new memory address. int[] arr1 = {1, 2, 3}; int[] arr2 = new int[arr1.length]; for (int i = 0; i \u003c arr1.length; ++i) { arr2[i] = arr1[i]; } arr2 is a deep copy of arr1. It occupies more memory space. ","date":"2020-01-21","objectID":"/posts/deep-copy-shallow-copy.html/:0:2","tags":["Programming"],"title":"Deep Copy and Shallow Copy","uri":"/posts/deep-copy-shallow-copy.html/"},{"categories":["C++"],"content":"Introduction C++ provides various containers to store and manage data, each with its unique characteristics and applicable scenarios. This article will delve into five common sequence containers: array, vector, deque, list, and forward_list, comparing their features, performance, and offering selection advice. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:1:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Array std::array is a fixed-size array introduced in C++11, combining the performance of C-style arrays with the interface of STL containers. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:2:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Characteristics Fixed Size: Size is determined at compile time and cannot be changed dynamically. Stack Allocation: Typically allocates memory on the stack, offering high performance. Random Access: Supports O(1) time complexity for random access. Iterator Support: Provides iterators, allowing use with STL algorithms. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:2:1","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"When to Use When the data size is known and fixed. When pursuing ultimate performance, avoiding heap allocation overhead. When interoperability with C-style arrays is required. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:2:2","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Example #include \u003carray\u003e #include \u003ciostream\u003e #include \u003cnumeric\u003e // For std::accumulate int main() { std::array\u003cint, 5\u003e my_array = {1, 2, 3, 4, 5}; // Access elements std::cout \u003c\u003c \"Element at index 2: \" \u003c\u003c my_array[2] \u003c\u003c std::endl; // Output: 3 // Iteration for (int\u0026 x : my_array) { x *= 2; } // Sum int sum = std::accumulate(my_array.begin(), my_array.end(), 0); std::cout \u003c\u003c \"Sum of elements: \" \u003c\u003c sum \u003c\u003c std::endl; // Output: 30 return 0; } ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:2:3","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Vector std::vector is a dynamic array that can change its size dynamically at runtime. It is the most commonly used and flexible sequence container in C++. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:3:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Concept Allocate array dynamically. When the capacity is not big enough, reallocate a new array with sufficient memory space and move elements to the new one. The actual capacity usually a bit bigger than the number of elements in the vector. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:3:1","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Characteristics Dynamic Size: Can automatically expand or shrink at runtime. Contiguous Memory: Elements are stored contiguously in memory, supporting efficient random access with O(1) time complexity. Automatic Memory Management: Automatically handles memory allocation and deallocation. Efficient Back Operations: Inserting and deleting elements at the back is typically O(1) amortized time complexity. Inefficient Middle Insertions/Deletions: Inserting or deleting elements in the middle requires moving many elements, resulting in O(n) time complexity. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:3:2","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"When to Use When the data size is unknown or changes dynamically. When frequent additions or deletions at the back are needed. When random access to elements is required. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:3:3","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Example #include \u003cvector\u003e #include \u003ciostream\u003e int main() { std::vector\u003cint\u003e my_vector; // Add elements my_vector.push_back(10); my_vector.push_back(20); my_vector.push_back(30); // Access elements std::cout \u003c\u003c \"Element at index 1: \" \u003c\u003c my_vector[1] \u003c\u003c std::endl; // Output: 20 // Iteration for (int x : my_vector) { std::cout \u003c\u003c x \u003c\u003c \" \"; // Output: 10 20 30 } std::cout \u003c\u003c std::endl; // Delete back element my_vector.pop_back(); // Size std::cout \u003c\u003c \"Vector size: \" \u003c\u003c my_vector.size() \u003c\u003c std::endl; // Output: 2 return 0; } ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:3:4","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Deque std::deque (double-ended queue) is a double-ended queue that supports efficient insertion and deletion of elements at both ends. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:4:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Characteristics Dynamic Size: Can automatically expand or shrink at runtime. Efficient Operations at Both Ends: Inserting and deleting elements at both the front and back are O(1) time complexity. For other elements that are not at the front or back, inserting and deleting are bit slower. Random Access: Supports O(1) time complexity for random access, but typically slower than vector. Non-contiguous Memory: Elements are not guaranteed to be stored contiguously in memory, usually composed of multiple fixed-size blocks. Leads to more efficiency for reallocation ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:4:1","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"When to Use When frequent additions or deletions at both ends are needed. When random access to elements is required, but performance requirements are not as strict as vector. As the underlying implementation for queues or stacks. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:4:2","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Example #include \u003cdeque\u003e #include \u003ciostream\u003e int main() { std::deque\u003cint\u003e my_deque; // Add to front my_deque.push_front(10); // Add to back my_deque.push_back(20); my_deque.push_back(30); // Access elements std::cout \u003c\u003c \"First element: \" \u003c\u003c my_deque.front() \u003c\u003c std::endl; // Output: 10 std::cout \u003c\u003c \"Element at index 1: \" \u003c\u003c my_deque[1] \u003c\u003c std::endl; // Output: 20 // Iteration for (int x : my_deque) { std::cout \u003c\u003c x \u003c\u003c \" \"; // Output: 10 20 30 } std::cout \u003c\u003c std::endl; // Delete from front my_deque.pop_front(); // Delete from back my_deque.pop_back(); std::cout \u003c\u003c \"Deque size: \" \u003c\u003c my_deque.size() \u003c\u003c std::endl; // Output: 1 return 0; } ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:4:3","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"List std::list is a doubly linked list that supports efficient insertion and deletion of elements at any position. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:5:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Characteristics Efficient Operations at Any Position: Inserting and deleting elements at any position are O(1) time complexity. Good choice for sorting Efficient iterator: Move backward or forward efficiently with doubly linked list Slow Random Access: Can only be accessed sequentially via iterators, with O(n) time complexity. Non-contiguous Memory: Elements are not stored contiguously in memory; each element contains pointers to the previous and next elements. Additional Memory Overhead: Each element requires extra memory to store pointers. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:5:1","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"When to Use When frequent insertions or deletions at any position are needed. When random access to elements is not required. When not sensitive to memory overhead. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:5:2","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Example #include \u003clist\u003e #include \u003ciostream\u003e #include \u003calgorithm\u003e // For std::find int main() { std::list\u003cint\u003e my_list; // Add elements my_list.push_back(10); my_list.push_back(20); my_list.push_back(40); // Insert in the middle auto it = std::find(my_list.begin(), my_list.end(), 20); my_list.insert(it, 30); // Iteration for (int x : my_list) { std::cout \u003c\u003c x \u003c\u003c \" \"; // Output: 10 20 30 40 } std::cout \u003c\u003c std::endl; // Delete element my_list.remove(20); std::cout \u003c\u003c \"List size: \" \u003c\u003c my_list.size() \u003c\u003c std::endl; // Output: 3 return 0; } ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:5:3","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Forward_list std::forward_list is a singly linked list, a lightweight version of std::list that only supports forward traversal. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:6:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Characteristics Forward Traversal Only: Can only traverse from head to tail. Efficient Operations at Any Position: Inserting and deleting elements at any position are O(1) time complexity (requires an iterator to the element before the insertion/deletion point). No Random Access Support: Can only be accessed sequentially via iterators, with O(n) time complexity. Smaller Memory Overhead: Each element contains only one pointer to the next element, saving memory compared to list. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:6:1","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"When to Use When only forward traversal is needed. When frequent insertions or deletions at any position are needed. When extremely sensitive to memory overhead. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:6:2","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Example #include \u003cforward_list\u003e #include \u003ciostream\u003e int main() { std::forward_list\u003cint\u003e my_forward_list; // Add elements (Note: forward_list does not have push_back) my_forward_list.push_front(40); my_forward_list.push_front(30); my_forward_list.push_front(10); // Insert after a specified position auto it = my_forward_list.begin(); // Points to 10 it++; // Points to 30 my_forward_list.insert_after(it, 20); // Insert 20 after 30 // Iteration for (int x : my_forward_list) { std::cout \u003c\u003c x \u003c\u003c \" \"; // Output: 10 30 20 40 } std::cout \u003c\u003c std::endl; // Delete element (requires an iterator to the element before) my_forward_list.remove(30); // Iteration for (int x : my_forward_list) { std::cout \u003c\u003c x \u003c\u003c \" \"; // Output: 10 20 40 } std::cout \u003c\u003c std::endl; return 0; } ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:6:3","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Summary and Selection Advice Feature \\ Container Array Vector Deque List Forward_list Size Fixed Dynamic Dynamic Dynamic Dynamic Memory Allocation Stack Heap Heap Heap Heap Memory Contiguous Yes Yes No No No Random Access O(1) O(1) O(1) O(n) O(n) Front Insert/Delete O(n) O(n) O(1) O(1) O(1) Back Insert/Delete O(n) O(1) O(1) O(1) O(n) (no push_back) Middle Insert/Delete O(n) O(n) O(n) O(1) O(1) Additional Overhead None Small Small Large Small ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:7:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"Selection Advice Array: When data size is fixed and known, and highest performance is desired. Vector: The most common and versatile choice. When data size changes dynamically, and operations are primarily at the back with a need for random access. Deque: When frequent insertions/deletions at both ends are needed, and random access is also required. List: When frequent insertions/deletions at any position are needed, and random access is not required. Forward_list: When all conditions for List are met, and only forward traversal is needed, with extreme sensitivity to memory overhead. ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:7:1","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["C++"],"content":"References C++ Reference - Containers GeeksforGeeks - C++ STL Containers http://www.cplusplus.com/reference/array/array/ http://www.cplusplus.com/reference/vector/vector/ http://www.cplusplus.com/reference/deque/deque/ http://www.cplusplus.com/reference/list/list/ http://www.cplusplus.com/reference/forward_list/forward_list/ ","date":"2020-01-12","objectID":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/:8:0","tags":["Programming","C++","STL","container"],"title":"C++ Container Characteristics and Usage Scenarios - array, vector, deque, list, forward_list","uri":"/posts/c-stl-container-compare-array-vector-dequeue-list-forward_list.html/"},{"categories":["Docker"],"content":"Continuing from Docker Operations Log (Part 1) ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:0:0","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Docker"],"content":"Basic Docker Usage ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:1:0","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Docker"],"content":"Deleting a Container Remember to stop the container using stop before deleting it. docker rm CONTAINER_NAME Or docker rm CONTAINER_ID After deletion, you can use docker ps -a to confirm if the container has disappeared. ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:1:1","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Docker"],"content":"Creating an Image from a Previously Exported Container If you previously exported a container as c_test.tar, you can use it to create a new image: cat c_test.tar | docker import - ubuntu_test_repo:1.0 ubuntu_test_repo is the repository name, and 1.0 is the tag. You can use docker images to list and check it. Once you have an image, you can create new containers from it. ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:1:2","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Docker"],"content":"Deleting an Image If I use docker images and the listed images are: REPOSITORY TAG IMAGE ID CREATED SIZE aaa 2.0 b30c39fffb75 4 seconds ago 64.2MB aaa 1.0 6b8046192d83 8 seconds ago 64.2MB ubuntu_test_repo 1.0 864c36a752c3 5 hours ago 64.2MB ubuntu latest 549b9b86cb8d 2 weeks ago 1.84kB hello-world latest fce289e99eb9 12 months ago 1.84kB To delete the image with repository name aaa and tag 1.0: docker rmi aaa:1.0 This will work. All containers using this image must be rm’d first. ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:1:3","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Docker"],"content":"Dockerfile A Dockerfile is a file that allows users to create images in a simpler way. It is divided into four parts: Image Maintainer (who is responsible for this Dockerfile) Operation commands Command to run when the container starts Here’s an Nginx example: # This is how to comment in a Dockerfile # Image FROM ubuntu # Maintainer MAINTAINER user user@example.com # Operation commands RUN apt-get update \\ \u0026\u0026 apt-get upgrade -y \\ \u0026\u0026 apt-get install -y nginx # Container Start Command CMD [\"nginx\", \"-g\", \"daemon off;\"] ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:2:0","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Docker"],"content":"Building an Image You can use docker build to create an image. If the Nginx Dockerfile mentioned above is located at /tmp/d_file and named test_d_file, to build it into an image and tag it as test-nginx-img/1.0: docker build -t test-nginx-img/1.0 -f /tmp/d_file/test_d_file . After building, check with docker images: REPOSITORY TAG IMAGE ID CREATED SIZE test-nginx-img/1.0 latest 7293588d00a9 27 seconds ago 152MB ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:3:0","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Docker"],"content":"Reference Docker docs ","date":"2020-01-09","objectID":"/posts/docker-operating-2.html/:4:0","tags":["Docker","Virtual Environment"],"title":"Docker Operations Log (Part 2)","uri":"/posts/docker-operating-2.html/"},{"categories":["Web Hosting"],"content":"When designing systems with higher traffic, you will eventually encounter cluster-related issues. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:0:0","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"Cluster A collection of one or more machines (nodes) with three different purposes: ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:1:0","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"Load Balancing Allows multiple machines to share tasks as evenly as possible, accelerating application execution. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:1:1","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"High Availability (HA) For high availability and redundancy, if one machine suddenly fails, others can take over. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:1:2","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"High Performance Computing High-performance/parallel computing systems, abbreviated as HPC clusters, combine the hardware of multiple machines to increase computing power, used to solve tasks that a single machine cannot handle. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:1:3","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"HA Operating Modes There are many types, such as N+1, N+M, … But the most common is a two-node cluster. A two-node cluster has two operating modes: Active-Passive Active-Active ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:2:0","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"Active-Passive (AP) A master-slave design. Under normal circumstances, only the master (Active) provides the service. When the master (Active) encounters a problem, the slave (Passive) takes over. Once the master (Active) recovers, it switches back, and the master (Active) continues to handle the service. Advantages: Fast fail-over speed. Relatively simple design and configuration. Disadvantages: Cannot perform load balancing simultaneously, wasting some hardware resources. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:2:1","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"Active-Active (AA) Both machines simultaneously run their own independent services (both are Active), and also provide mutual redundancy (acting as the other’s Passive). When one machine encounters a problem, the other takes over its service. Advantages: Neither machine is idle during normal operation, resulting in high operational efficiency. Disadvantages: The machine’s load increases after fail-over, leading to slower performance. Relatively complex design and configuration. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:2:2","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"Application Design There needs to be a relatively simple way to start, stop, force-stop services, and check the current status of services. =\u003e When designing the application, there should be a command-line interface or script to achieve this. =\u003e Services on both machines should be able to know each other’s status and be able to start or stop in case of an accident. Shared storage is required, and the application should record its state as meticulously as possible to shared storage. =\u003e This ensures nothing is lost when switching between the two machines. It should be possible to restart another node and restore it to the state before the failure occurred. =\u003e Restoring to the pre-failure state can be done using the state saved to shared storage. When the application crashes, the data stored on shared storage must not be corrupted. =\u003e The other side needs to use it. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:3:0","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"Remark Consider scenarios that occur during application upgrades. Some SQL or NoSQL databases inherently support these types of configurations, which can be adopted to reduce a lot of trouble. ","date":"2020-01-09","objectID":"/posts/ha-cluster-app-architecture.html/:3:1","tags":["Cluster","High availability","Architecture"],"title":"HA Cluster Notes and Application Design","uri":"/posts/ha-cluster-app-architecture.html/"},{"categories":["Web Hosting"],"content":"Each DNS zone in a DNS server has a zone file. A DNS zone is usually a single domain (though not always). A zone file is composed of many DNS resource records (RRs). There are many different types of RRs. Let’s record some common ones. ","date":"2020-01-08","objectID":"/posts/common-dns-resource-record.html/:0:0","tags":["DNS"],"title":"Common DNS Resource Records","uri":"/posts/common-dns-resource-record.html/"},{"categories":["Web Hosting"],"content":"A record Maps a hostname to an IPv4 address. (32-bit) hostname IN A xxx.xxx.xxx.xxx ","date":"2020-01-08","objectID":"/posts/common-dns-resource-record.html/:1:0","tags":["DNS"],"title":"Common DNS Resource Records","uri":"/posts/common-dns-resource-record.html/"},{"categories":["Web Hosting"],"content":"AAAA record Maps a hostname to an IPv6 address. (128-bit) hostname IN AAAA xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx ","date":"2020-01-08","objectID":"/posts/common-dns-resource-record.html/:2:0","tags":["DNS"],"title":"Common DNS Resource Records","uri":"/posts/common-dns-resource-record.html/"},{"categories":["Web Hosting"],"content":"CNAME record An alias for a hostname. alias IN CNAME hostname Note that an alias cannot have other A records or MX records. ","date":"2020-01-08","objectID":"/posts/common-dns-resource-record.html/:3:0","tags":["DNS"],"title":"Common DNS Resource Records","uri":"/posts/common-dns-resource-record.html/"},{"categories":["Web Hosting"],"content":"MX record Mail exchanger record. The email server’s domain name, priority, and hostname. Note that it must be a hostname, not directly an IP address, and it cannot be a CNAME alias. Therefore, an additional RR (A or AAAA record) must be set up for the IP. A lower priority number indicates higher priority (mail is delivered to higher priority servers first). mail_domain_name IN MX priority hostname Example: example.com. IN MX 10 mailserver.example.com","date":"2020-01-08","objectID":"/posts/common-dns-resource-record.html/:4:0","tags":["DNS"],"title":"Common DNS Resource Records","uri":"/posts/common-dns-resource-record.html/"},{"categories":["Database"],"content":"RDBMS Relational Database Management System Used when there are strong Relations between data: Design a schema that is unlikely to change, relating tables to each other, then you can retrieve the desired data through SQL. Used when data correctness is very important: Usually provides ACID properties. Changing the schema is a huge undertaking: Requires updating the table schema and migrating data. All programs that use the table with the changed schema need to be modified. Vertical scaling is more effective (improving machine performance). ","date":"2020-01-06","objectID":"/posts/rdbms-acid-nosql-cap.html/:1:0","tags":["Database","NoSQL","RDBMS"],"title":"RDBMS and NoSQL Differences Notes","uri":"/posts/rdbms-acid-nosql-cap.html/"},{"categories":["Database"],"content":"ACID RDBMS usually guarantees four properties for transactions: Atomicity Only two possibilities: all completed (Commit) or all not done (Abort). There is no “half-done” state. If there is an error during execution, it will Rollback to the state where nothing was done. Consistency The database will remain in a legal state before and after the transaction. Isolation When multiple transactions need to be executed, each transaction is separate and does not interfere with each other. Transaction A and B does not affect transaction B and C. Durability Once a transaction is completed, it is permanently valid and will not be lost, even if the system suddenly fails. ","date":"2020-01-06","objectID":"/posts/rdbms-acid-nosql-cap.html/:1:1","tags":["Database","NoSQL","RDBMS"],"title":"RDBMS and NoSQL Differences Notes","uri":"/posts/rdbms-acid-nosql-cap.html/"},{"categories":["Database"],"content":"NoSQL Not only SQL Less concerned with relations between data: Does not require a fixed schema for data access. Each piece of data exists independently, without issues of who relates to whom. More concerned with the content of the data: Whether updates, additions, deletions, etc., are needed. Data can have different formats. More suitable for distributed systems. Usually provides two of the CAP properties. Horizontal scaling is more effective (adding more machines). ","date":"2020-01-06","objectID":"/posts/rdbms-acid-nosql-cap.html/:2:0","tags":["Database","NoSQL","RDBMS"],"title":"RDBMS and NoSQL Differences Notes","uri":"/posts/rdbms-acid-nosql-cap.html/"},{"categories":["Database"],"content":"CAP For a distributed system, it is impossible to guarantee all three CAP properties simultaneously (though they might coexist when the network is stable). At most, two can be guaranteed simultaneously. Consistency Every read, if it doesn’t result in an error, will return the result of the most recent write. =\u003e Data on every node is identical. Availability Every request will receive a non-error response, regardless of whether the data returned by this response is the latest. =\u003e Guarantees that data will always be returned, but the data might be old. Partition tolerance Even if some messages transmitted between nodes are delayed or lost, the system will continue to operate. =\u003e When network issues occur, the normally connected part of the nodes can continue to operate. ","date":"2020-01-06","objectID":"/posts/rdbms-acid-nosql-cap.html/:2:1","tags":["Database","NoSQL","RDBMS"],"title":"RDBMS and NoSQL Differences Notes","uri":"/posts/rdbms-acid-nosql-cap.html/"},{"categories":["Database"],"content":"Reference Wiki ACID Wiki CAP ","date":"2020-01-06","objectID":"/posts/rdbms-acid-nosql-cap.html/:3:0","tags":["Database","NoSQL","RDBMS"],"title":"RDBMS and NoSQL Differences Notes","uri":"/posts/rdbms-acid-nosql-cap.html/"},{"categories":["Operating System"],"content":"To understand what a daemon is, I consulted The Linux Programming Interface and took notes related to it. A process group is a collection of related processes. A session is a collection of related process groups. Process groups and sessions are defined for job control ","date":"2020-01-02","objectID":"/posts/linux-process-group-sessions-daemon.html/:0:0","tags":["Linux","OS"],"title":"Process Groups, Sessions and Daemon Overview","uri":"/posts/linux-process-group-sessions-daemon.html/"},{"categories":["Operating System"],"content":"Process Group A process group is a collection of related processes that share the same process group identifier (PGID). Each process group has a process group leader, which is the process that created the group. The PGID of the process group is the PID of its leader. The PGID of any newly created process is inherited from its parent. The lifetime of a process group begins when its leader creates it and ends when all processes have left the group. A process ’leaves’ a process group either by terminating or by being switched to another process group. ","date":"2020-01-02","objectID":"/posts/linux-process-group-sessions-daemon.html/:1:0","tags":["Linux","OS"],"title":"Process Groups, Sessions and Daemon Overview","uri":"/posts/linux-process-group-sessions-daemon.html/"},{"categories":["Operating System"],"content":"Session A session is a collection of related process groups that share the same session identifier (SID). Each session has a session leader, which is the process that created the session. The SID of the session is the PID of its leader. Whenever a new process is created, its SID is inherited from its parent. All processes in the same session share a controlling terminal. A controlling terminal is established the first time a session leader opens a terminal device, and a terminal can only be the controlling terminal for one session. Hence, there is a one-to-one relationship between a session and a controlling terminal. At any given time, there are: foreground process group: A process group within a session. Only processes in this process group can receive input from the controlling terminal. background process groups: All process groups that are not the foreground process group. ","date":"2020-01-02","objectID":"/posts/linux-process-group-sessions-daemon.html/:2:0","tags":["Linux","OS"],"title":"Process Groups, Sessions and Daemon Overview","uri":"/posts/linux-process-group-sessions-daemon.html/"},{"categories":["Operating System"],"content":"Terminal When a terminal is opened, a session leader is established, and this same session leader also acts as the controlling process. Meanwhile, the foreground process group waits for input from the terminal, which could be user input or signals from the user, while background process groups exist concurrently. When a terminal is terminated, the kernel sends a SIGHUP signal to the session leader to indicate that the terminal session has ended. ","date":"2020-01-02","objectID":"/posts/linux-process-group-sessions-daemon.html/:3:0","tags":["Linux","OS"],"title":"Process Groups, Sessions and Daemon Overview","uri":"/posts/linux-process-group-sessions-daemon.html/"},{"categories":["Operating System"],"content":"Shell Job Control Process groups and sessions are defined to explain shell job control. Here is an example of shell job control: The terminal used by the user to log in is the controlling terminal, and the login shell acts as both the session leader and the controlling process. Commands executed from this shell will create one or more processes. These processes form new process groups, and any other processes created by them become part of these process groups. All these processes are created from this shell and thus belong to this login session. ","date":"2020-01-02","objectID":"/posts/linux-process-group-sessions-daemon.html/:4:0","tags":["Linux","OS"],"title":"Process Groups, Sessions and Daemon Overview","uri":"/posts/linux-process-group-sessions-daemon.html/"},{"categories":["Operating System"],"content":"Daemon Characteristics of daemons: long-lived: Usually activated when the system starts, running until the system is turned off. Running in the background and having no controlling terminal: To ensure that no job control or terminal-related signals generated by the kernel affect the daemon. Daemons usually end with ’d'. Several common daemons: cron sshd httpd ","date":"2020-01-02","objectID":"/posts/linux-process-group-sessions-daemon.html/:5:0","tags":["Linux","OS"],"title":"Process Groups, Sessions and Daemon Overview","uri":"/posts/linux-process-group-sessions-daemon.html/"},{"categories":["Docker"],"content":"Basic Concept Docker can be seen as a simplified virtual machine (VM). Since it doesn’t install a full operating system, it offers a smaller footprint and faster speed. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:1:0","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Image An Image contains a lightweight runtime environment, including its libraries and executables. Images can be thought of as the .iso file for a Docker VM. It can only be read, not executed directly. If one wants to modify an image, they can only create a new image based on the existing one. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:1:1","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Container When an image is used to create a running environment, it becomes a container. Like a VM, the Docker container is isolated from the host environment. Whatever is done within the container does not affect the host environment, unless specific settings are configured. For example, We can expose a port of a Docker container, while the host port remains closed. However, we can also expose the host’s port if desired. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:1:2","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Repository A repository is where images are stored. It is similar to a Git repository: There can be many repositories, and each serves as a place to store the code of a project. Similarly, Docker repositories are where images are stored. Every image within the same repository shares the same name but has different tags. Additionally, there are many different repositories for various images. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:1:3","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Registry A registry is also a place to store images. The difference between a registry and a repository is that a registry is a service where users can push or pull images to and from their local machines, similar to GitHub for Git. The most famous one is Docker Hub. Whereas a repository is a location to keep images with the same name but different tags. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:1:4","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Basic Usage of Docker ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:0","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Install It is very simple in Ubuntu: sudo apt-get install docker.io ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:1","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Pull Image There are many images on Docker Hub that can be used. If I want a clean Ubuntu environment, I can pull an Ubuntu image to my local machine: docker pull ubuntu Or if I want to specify a tag: docker pull ubuntu:14.04 ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:2","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Run the Image We can echo Hello world using the image we just obtained: docker run ubuntu /bin/echo 'Hello world' This should print Hello world on the terminal. What just happened is that the docker run command creates a temporary container, and terminates itself after the echo command completes. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:3","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"List Images at Local docker images It should list the image we just pulled to our local machine. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:4","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Create a Container Once we have a container, we have a running environment that can be modified by our actions. Creating a container from an image is just like using an .iso to create a virtual machine. We can create a container running Ubuntu with the image we pulled: docker create -it ubuntu We can also create a container with a name: docker create -it --name CONTAINER_NAME ubuntu i refers to ‘interactive’ (opens stdin of the container). t refers to ‘TTY’ (allocates a pseudo-TTY so we can interact with it via a terminal). Or if we want to create a container and run it: docker run -itd ubuntu or docker run -itd --name CONTAINER_NAME ubuntu d refers to ‘detach’ (runs the container in the background). ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:5","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"List Containers docker ps -a This should list all containers on the host machine. And we can observe a difference in status between containers created with docker create and docker run: Containers created with docker create are only created and not yet running, so their status is created; whereas containers created with docker run are both created and run, so their status is up. There is a container ID that can be used to run or terminate the container. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:6","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Run Containers So, if a container is created with docker create, it must be run before we can access it: We can use the container ID to run the container: docker start \"CONTAINER_ID\" Or, if the container was created with a name, we can use that name to run it: docker start \"CONTAINER_NAME\" If the container’s status is exited, it also needs to be started to run before we can access it. Use docker ps -a to check the status first. For containers created with docker run, or those already started with docker start, we can access them with docker exec: docker exec -it \"CONTAINER_ID\" bash bash is the command we want to run; it can be replaced with other commands like echo or anything else. We can also use the container’s name to access it: docker exec -it \"CONTAINER_NAME\" bash If bash is the command used, we should find ourselves inside the container. The user becomes root, and we can start configuring settings or installing software within the container. If we want to leave the container: exit The container remains running in the background after we exit it. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:7","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Stop Containers This is quite similar to turning off a virtual machine. Stopping a container only changes its status to exited; it does not remove the container entirely. docker stop \"CONTAINER_ID\" Or, docker stop \"CONTAINER_NAME\" If we inspect it with, docker ps -a We will find that the container still exists, but its status has changed to exited. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:8","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Export Container Once a container is exported, it can be moved to another host machine. We can export a container as a .tar file. For example, export a container as exported.tar: docker export \"CONTAINER_ID\" \u003e exported.tar Or, docker export \"CONTAINER_NAME\" \u003e exported.tar Then we can move the .tar file to another machine. ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:2:9","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Docker"],"content":"Reference Docker docs ","date":"2020-01-01","objectID":"/posts/docker-operating-1.html/:3:0","tags":["Docker","Virtual Environment"],"title":"Docker Notes 1 - Beginner","uri":"/posts/docker-operating-1.html/"},{"categories":["Blog"],"content":"Update I’ve moved from Jekyll to Hugo. This method is only applicable to Jekyll. ","date":"2019-12-30","objectID":"/posts/jekyll-sitemap-github-pages.html/:1:0","tags":["Programming","Jekyll","SEO","Github Pages"],"title":"Github Pages and Jekyll - sitemap","uri":"/posts/jekyll-sitemap-github-pages.html/"},{"categories":["Blog"],"content":"Sitemap A sitemap is an .xml file that contains links to all the pages within a website. With a sitemap, a search engine can discover the pages and subsequently create indexes for them. Then, people browsing the internet can find those pages using keywords. ","date":"2019-12-30","objectID":"/posts/jekyll-sitemap-github-pages.html/:2:0","tags":["Programming","Jekyll","SEO","Github Pages"],"title":"Github Pages and Jekyll - sitemap","uri":"/posts/jekyll-sitemap-github-pages.html/"},{"categories":["Blog"],"content":"Jekyll-sitemap There is a plugin called jekyll-sitemap for Jekyll, which automatically generates a sitemap whenever the website is rebuilt. It is a good choice if you build your website locally, but with GitHub Pages, it doesn’t work as expected. It’s unclear if this is due to the parameters or how GitHub builds websites; The sitemap is generated, but the URLs are incorrect. ","date":"2019-12-30","objectID":"/posts/jekyll-sitemap-github-pages.html/:3:0","tags":["Programming","Jekyll","SEO","Github Pages"],"title":"Github Pages and Jekyll - sitemap","uri":"/posts/jekyll-sitemap-github-pages.html/"},{"categories":["Blog"],"content":"Generates sitemap without plugin So I found this, which seemed to work, so I decided to give it a try, modified it, and placed it in sitemap.xml inside the repository, and it did work! ","date":"2019-12-30","objectID":"/posts/jekyll-sitemap-github-pages.html/:4:0","tags":["Programming","Jekyll","SEO","Github Pages"],"title":"Github Pages and Jekyll - sitemap","uri":"/posts/jekyll-sitemap-github-pages.html/"},{"categories":["C++"],"content":"const with Normal Variables Two ways to add const for normal variables: const TYPE NAME = VALUE; // more common TYPE const NAME = VAULE; Both mean this variable cannot be assigned to another value. For example, #include \u003ciostream\u003e using namespace std; int main(void) { const int i = 1; int const j = 1; i = 2; // error j = 2; // error cout \u003c\u003c \"i = \" \u003c\u003c i \u003c\u003c endl; cout \u003c\u003c \"j = \" \u003c\u003c j \u003c\u003c endl; return 0; } The same error occurs for i and j: const.cpp:9:4: error: cannot assign to variable 'i' with const-qualified type 'const int' i = 2; ~ ^ const.cpp:7:12: note: variable 'i' declared const here const int i = 1; ~~~~~~~~~~^~~~~ const.cpp:10:4: error: cannot assign to variable 'j' with const-qualified type 'const int' j = 2; ~ ^ const.cpp:8:12: note: variable 'j' declared const here int const j = 1; ~~~~~~~~~~^~~~~ 2 errors generated. ","date":"2019-12-30","objectID":"/posts/const-pointer-reference.html/:1:0","tags":["Programming","C","C++"],"title":"C/C++ - const with Pointer or Reference","uri":"/posts/const-pointer-reference.html/"},{"categories":["C++"],"content":"const and Reference There are also two ways to add const to a reference: const TYPE \u0026NAME = VALUE; // more common TYPE const \u0026NAME = VAULE; Both have the same meaning. There are two restrictions for them: This reference cannot be reassigned to another variable The variable being referenced cannot have its value changed through this reference, but its value can be changed without using this reference. For example, #include \u003ciostream\u003e using namespace std; int main(void) { int i = 1, j = 2; int const \u0026r1 = i; const int \u0026r2 = i; // change value with reference r1 = 3; // error r2 = 3; // error // change value i = 4; // change reference object r1 = j; // error r2 = j; // error return 0; } A constant reference can only be read. If the value of the variable it references has been changed, it can only be changed without using that reference. ","date":"2019-12-30","objectID":"/posts/const-pointer-reference.html/:2:0","tags":["Programming","C","C++"],"title":"C/C++ - const with Pointer or Reference","uri":"/posts/const-pointer-reference.html/"},{"categories":["C++"],"content":"const and Pointer This can be complicated. However, we can use the position of const to determine what it is modifying: TYPE* const pNAME; // 1 TYPE const *pNAME; // 2 const TYPE *pNAME; // 3 const TYPE* const pNAME; // 4 For 1, const modifies pNAME, meaning that pNAME cannot be changed (i.e., pNAME = ... is not allowed). For 2, const modifies *pNAME, so the value pointed to by pNAME cannot be changed (i.e., *pNAME = ... is not allowed). For 3, const modifies TYPE *pNAME. This is the same as case 2, meaning that the value pointed to by pNAME cannot be changed (i.e., *pNAME = ... is not allowed). For 4, const modifies both pNAME and the TYPE it points to, so neither pNAME nor the value it points to can be changed (i.e., pNAME = ... or *pNAME = ... are not allowed). #include \u003ciostream\u003e using namespace std; int main(void) { int i = 1, j = 2; int* const p1 = \u0026i; int const *p2 = \u0026i; const int *p3 = \u0026i; const int* const p4 = \u0026i; // Change value through pointer *p1 = 2; *p2 = 2; // error *p3 = 2; // error *p4 = 2; // error // change value i = 3; // Change pointer's target p1 = \u0026j; // error p2 = \u0026j; p3 = \u0026j; p4 = \u0026j; // error return 0; }","date":"2019-12-30","objectID":"/posts/const-pointer-reference.html/:3:0","tags":["Programming","C","C++"],"title":"C/C++ - const with Pointer or Reference","uri":"/posts/const-pointer-reference.html/"},{"categories":["Blog"],"content":"Update I’ve moved my blog from Jekyll to Hugo. The method for adding a like button remains similar, but the code and its placement need to be adjusted. ","date":"2019-12-27","objectID":"/posts/likecoin-button-jekyll.html/:1:0","tags":["Likecoin","Frontend","Github Pages"],"title":"Add LikeWidget to Jekyll theme","uri":"/posts/likecoin-button-jekyll.html/"},{"categories":["Blog"],"content":"LikeCoin I came across LikeCoin and it piqued my interest. LikeCoin is a cryptocurrency, which was created to encourage content creators. Content creators can embed a Like Button within their content or web pages. Anyone with a LikeCoin account who appreciates the content can click the button, and the content creator will receive the corresponding LikeCoin. The amount of LikeCoin a content creator receives depends on the account type of the likers. The LikeCoin Foundation proportionally rewards free account likers, while payment accounts are proportionally charged based on the number of likes they click each month. More details can be found on LikeCoin’s Medium. ","date":"2019-12-27","objectID":"/posts/likecoin-button-jekyll.html/:2:0","tags":["Likecoin","Frontend","Github Pages"],"title":"Add LikeWidget to Jekyll theme","uri":"/posts/likecoin-button-jekyll.html/"},{"categories":["Blog"],"content":"Like Rewards Button for Jekyll Theme So I registered an account and found that the like button widget supports platforms like Medium, WordPress, Oice, Matters, etc. However, since Jekyll themes are custom-designed, we need to manually integrate the Like button. So I found an article with an embedded Like button on Medium and inspected it using my browser’s developer console: It appears to be using an iframe. To make it work, the src attribute needs to include our liker ID and the article address. So, there are two different ways to add a like button to the blog post: Add the iframe to every blog post Add the iframe to the template which generates the blog posts Clearly, the second approach is better. So I located the template that generates blog posts at _layouts/post.html in my Jekyll theme, and I added this code to the template after the content section: \u003cdiv align=\"center\"\u003e \u003ciframe scrolling=\"no\" src=\"https://button.like.co/in/embed/\u003cMY_LIKER_ID\u003e/button/?referrer={{ site.url }}{{ page.url }}\" frameborder=\"0\"\u003e\u003c/iframe\u003e \u003c/div\u003e Remember to replace the Liker ID with your own. site.url and page.url are Liquid syntax, representing the URL for the current blog post. After adding this, every generated blog post should have a like button at the end of the article. ","date":"2019-12-27","objectID":"/posts/likecoin-button-jekyll.html/:3:0","tags":["Likecoin","Frontend","Github Pages"],"title":"Add LikeWidget to Jekyll theme","uri":"/posts/likecoin-button-jekyll.html/"},{"categories":["Git"],"content":"Every time a new folder is created and files are added in macOS, a .DS_Store file is generated within that folder. This results in numerous .DS_Store files scattered across macOS, and it’s quite annoying to add **/.DS_Store to every .gitignore file each time a new Git repository is created. Therefore, I found a method to prevent .DS_Store files from being tracked in all Git repositories. ","date":"2019-12-25","objectID":"/posts/remove-ds_store-from-all-git-repo.html/:0:0","tags":["Git","gitignore","Tool"],"title":"Remove .DS_Store tracking in all Git repositories","uri":"/posts/remove-ds_store-from-all-git-repo.html/"},{"categories":["Git"],"content":"The git config command The git config command can be used to set a variety of Git settings. The most common uses are git config --global user.name and git config --global user.email, which set user.name and user.email globally, allowing all Git repositories to use these settings. This command can also be used to set local settings for a single Git repository. If we wish to use a different username or email within a specific repository, we can use git config --local user.name and git config --local user.email to achieve this. There is a core.excludesfile setting for git config, which can be set to an ignore configuration file to specify files that all Git repositories should ignore. Therefore, all we need to do is write .DS_Store and **/.DS_Store into a file, and then set core.excludesfile to point to that file. Then all Git repositories will ignore .DS_Store files. We can achieve this by using these commands: echo \".DS_Store\" \u003e\u003e ~/.gitignore_global echo \"**/.DS_Store\" \u003e\u003e ~/.gitignore_global git config --global core.excludesfile ~/.gitignore_global","date":"2019-12-25","objectID":"/posts/remove-ds_store-from-all-git-repo.html/:1:0","tags":["Git","gitignore","Tool"],"title":"Remove .DS_Store tracking in all Git repositories","uri":"/posts/remove-ds_store-from-all-git-repo.html/"},{"categories":["Concurrency"],"content":"Both ‘Concurrent Processing’ and ‘Parallel Processing’ refer to multiple processes executing on the CPU within a period, but they are two different things. According to The Art of Concurrency, Concurrent means: two or more processes are in progress at the same time While Parallel means: two or more processes executing simultaneously They look pretty similar but are actually different. For example, two processes are executing, process A and process B. ","date":"2019-12-25","objectID":"/posts/concurrent-process-parallel-process.html/:0:0","tags":["Programming","Concurrent Processing","Parallel Processing"],"title":"Difference between Concurrent Processing and Parallel Processing","uri":"/posts/concurrent-process-parallel-process.html/"},{"categories":["Concurrency"],"content":"Parallel Processing Parallel processing may look like this： Both Process A and Process B are being executed. ","date":"2019-12-25","objectID":"/posts/concurrent-process-parallel-process.html/:0:1","tags":["Programming","Concurrent Processing","Parallel Processing"],"title":"Difference between Concurrent Processing and Parallel Processing","uri":"/posts/concurrent-process-parallel-process.html/"},{"categories":["Concurrency"],"content":"Concurrent Processing However, for concurrent processing, the execution might look like the diagram above, or it might look like this: Both Process A and Process B are in progress, but they are not executing at the same time. ","date":"2019-12-25","objectID":"/posts/concurrent-process-parallel-process.html/:0:2","tags":["Programming","Concurrent Processing","Parallel Processing"],"title":"Difference between Concurrent Processing and Parallel Processing","uri":"/posts/concurrent-process-parallel-process.html/"},{"categories":["Concurrency"],"content":"Notice Parallel processing is only a type of concurrent processing. As long as there are multiple processes in progress, it is concurrent processing. There are many ways to achieve concurrent processing, and parallel processing is only one of them. ","date":"2019-12-25","objectID":"/posts/concurrent-process-parallel-process.html/:0:3","tags":["Programming","Concurrent Processing","Parallel Processing"],"title":"Difference between Concurrent Processing and Parallel Processing","uri":"/posts/concurrent-process-parallel-process.html/"}]